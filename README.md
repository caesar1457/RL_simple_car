# ğŸš— Deep Q-Network Navigation in PyBullet

This project builds upon the [fredsukkar/simple-car-env-template](https://github.com/fredsukkar/simple-car-env-template) and implements a **Deep Q-Network (DQN)** agent for autonomous navigation in a 2D **PyBullet** environment. The agent learns to reach dynamic goals while avoiding collisions, using **discrete actions**, **reward shaping**, and **visual diagnostics**.

> âœ… Extended and independently developed by Caesar Zhao (MSc Robotics, UTS)

---

## ğŸ¯ Project Highlights

- **Extended Car Environment**: Built on top of `simple-car-env-template`, with enhancements in random obstacle generation and evaluation.
- **Deep Q-Network (DQN)**: Discrete policy with state-action value function approximation.
- **Reward Shaping**: Encourages reaching goals and penalizes collisions.
- **Evaluation Metrics**: Includes success rate, average reward, steps-to-goal.
- **Visual Diagnostics**: Evaluation animations, reward curves, and exploration insights.

---

## ğŸ› ï¸ Environment Setup

```bash
conda create -n dqn_car python=3.10
conda activate dqn_car
pip install -r requirements.txt  # includes pybullet, gym, matplotlib, numpy, etc.
```

---

## â–¶ï¸ How to Run

1. **Train the Agent**

```bash
python train_dqn_agent.py
```

2. **Evaluate the Trained Policy**

```bash
python evaluate_agent.py
```

3. **Visualize Results**

- `evaluation_animation.gif`: Demo of a successful run
- `reward_curve.png`: Cumulative reward during training
- `epsilon_distribution.png`: Action exploration analysis

---

## ğŸ§  Methodology

- **Observation Space**: Local frame with relative goal and obstacles.
- **Action Space**: 9 discrete motion commands (linear & angular).
- **Reward Function**:
  - Positive reward on reaching goal
  - Penalties for collisions and prolonged paths
  - Shaped reward âˆ†d to encourage proximity to goal
- **Q-Network**:
  - MLP with 2 hidden layers of 128 units
  - Target network and experience replay
- **Exploration**: Îµ-greedy with decay

---

## ğŸ“Š Results (10 Evaluation Episodes)

| Metric | Value |
|--------|--------|
| **Success Rate** | 50% |
| **Avg Reward** | -38.23 |
| **Avg Steps to Goal** | 19.6 |

> Results indicate promising generalization under randomized obstacle setups.

---

## ğŸ“‚ Repository Structure

```
RL_simple_car/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ dqnpy.py               # Core DQN agent logic
â”‚   â”œâ”€â”€ evaluator.py           # Evaluation logic
â”‚   â”œâ”€â”€ main.py                # Entry point to training
â”‚   â””â”€â”€ report_charts.py       # Plotting training results
â”œâ”€â”€ simple_driving/
â”‚   â””â”€â”€ envs/simple_driving_env.py  # Modified PyBullet gym env
â”œâ”€â”€ resources/                 # URDFs and geometry
â”œâ”€â”€ data/                      # Saved figures and results
â”œâ”€â”€ videos/                    # Evaluation animations
â”œâ”€â”€ models/                    # Saved model checkpoints
â”œâ”€â”€ dqn_navigation_pybullet.pdf  # Final project report
â””â”€â”€ README.md                  # Project overview and usage
```

---

## ğŸ™ Acknowledgements

This project is based on the template from:
**fredsukkar/simple-car-env-template**  
ğŸ”— https://github.com/fredsukkar/simple-car-env-template

---

## ğŸ§‘â€ğŸ’» Author

**Zhiye (Caesar) Zhao**  
MSc Robotics, University of Technology Sydney  
ğŸ“§ [zhiye.zhao-1@student.uts.edu.au](mailto:zhiye.zhao-1@student.uts.edu.au)  
ğŸŒ [Portfolio Website](https://caesar1457.github.io/zhiyezhao/)

---

## ğŸ“œ License

This project is provided for academic demonstration purposes only.  
Â© 2024 Caesar Zhao. All rights reserved.

